
R Under development (unstable) (2021-05-10 r80280) -- "Unsuffered Consequences"
Copyright (C) 2021 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> pkgname <- "mlr"
> source(file.path(R.home("share"), "R", "examples-header.R"))
> options(warn = 1)
> library('mlr')
Loading required package: ParamHelpers
Warning message: 'mlr' is in 'maintenance-only' mode since July 2019.
Future development will only happen in 'mlr3'
(<https://mlr3.mlr-org.com>). Due to the focus on 'mlr3' there might be
uncaught bugs meanwhile in {mlr} - please consider switching.
> 
> base::assign(".oldSearch", base::search(), pos = 'CheckExEnv')
> base::assign(".old_wd", base::getwd(), pos = 'CheckExEnv')
> cleanEx()
> nameEx("FailureModel")
> ### * FailureModel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: FailureModel
> ### Title: Failure model.
> ### Aliases: FailureModel
> 
> ### ** Examples
> 
> configureMlr(on.learner.error = "warn")
> data = iris
> data$newfeat = 1 # will make LDA crash
> task = makeClassifTask(data = data, target = "Species")
> m = train("classif.lda", task) # LDA crashed, but mlr catches this
Warning in train("classif.lda", task) :
  Could not train learner classif.lda: Error in lda.default(x, grouping, ...) : 
  variable 5 appears to be constant within groups

> print(m)
Model for learner.id=classif.lda; learner.class=classif.lda
Trained on: task.id = data; obs = 150; features = 5
Hyperparameters: 
Training failed: Error in lda.default(x, grouping, ...) : 
  variable 5 appears to be constant within groups

Training failed: Error in lda.default(x, grouping, ...) : 
  variable 5 appears to be constant within groups

> print(m$learner.model) # the error message
[1] "Error in lda.default(x, grouping, ...) : \n  variable 5 appears to be constant within groups\n"
> p = predict(m, task) # this will predict NAs
> print(p)
Prediction: 150 observations
predict.type: response
threshold: 
time: NA
  id  truth response
1  1 setosa     <NA>
2  2 setosa     <NA>
3  3 setosa     <NA>
4  4 setosa     <NA>
5  5 setosa     <NA>
6  6 setosa     <NA>
... (#rows: 150, #cols: 3)
> print(performance(p))
mmce 
  NA 
> configureMlr(on.learner.error = "stop")
> 
> 
> 
> cleanEx()
> nameEx("Task")
> ### * Task
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Task
> ### Title: Create a classification, regression, survival, cluster,
> ###   cost-sensitive classification or multilabel task.
> ### Aliases: Task
> 
> ### ** Examples
> 
> if (requireNamespace("mlbench")) {
+   library(mlbench)
+   data(BostonHousing)
+   data(Ionosphere)
+ 
+   makeClassifTask(data = iris, target = "Species")
+   makeRegrTask(data = BostonHousing, target = "medv")
+   # an example of a classification task with more than those standard arguments:
+   blocking = factor(c(rep(1, 51), rep(2, 300)))
+   makeClassifTask(id = "myIonosphere", data = Ionosphere, target = "Class",
+     positive = "good", blocking = blocking)
+   makeClusterTask(data = iris[, -5L])
+ }
Loading required namespace: mlbench
Unsupervised task: iris[, -5L]
Type: cluster
Observations: 150
Features:
   numerics     factors     ordered functionals 
          4           0           0           0 
Missings: FALSE
Has weights: FALSE
Has blocking: FALSE
Has coordinates: FALSE
> 
> 
> 
> cleanEx()

detaching ‘package:mlbench’

> nameEx("benchmark")
> ### * benchmark
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: benchmark
> ### Title: Benchmark experiment for multiple learners and tasks.
> ### Aliases: benchmark
> 
> ### ** Examples
> 
> lrns = list(makeLearner("classif.lda"), makeLearner("classif.rpart"))
> tasks = list(iris.task, sonar.task)
> rdesc = makeResampleDesc("CV", iters = 2L)
> meas = list(acc, ber)
> bmr = benchmark(lrns, tasks, rdesc, measures = meas)
Task: iris-example, Learner: classif.lda
Resampling: cross-validation
Measures:             acc       ber       
[Resample] iter 1:    0.9600000 0.0401235 
[Resample] iter 2:    1.0000000 0.0000000 


Aggregated Result: acc.test.mean=0.9800000,ber.test.mean=0.0200617


Task: Sonar-example, Learner: classif.lda
Resampling: cross-validation
Measures:             acc       ber       
[Resample] iter 1:    0.6538462 0.3470218 
[Resample] iter 2:    0.7211538 0.2972264 


Aggregated Result: acc.test.mean=0.6875000,ber.test.mean=0.3221241


Task: iris-example, Learner: classif.rpart
Resampling: cross-validation
Measures:             acc       ber       
[Resample] iter 1:    0.9333333 0.0679012 
[Resample] iter 2:    0.8400000 0.1555184 


Aggregated Result: acc.test.mean=0.8866667,ber.test.mean=0.1117098


Task: Sonar-example, Learner: classif.rpart
Resampling: cross-validation
Measures:             acc       ber       
[Resample] iter 1:    0.6730769 0.3303737 
[Resample] iter 2:    0.7307692 0.2953523 


Aggregated Result: acc.test.mean=0.7019231,ber.test.mean=0.3128630


> rmat = convertBMRToRankMatrix(bmr)
> print(rmat)
              Sonar-example iris-example
classif.lda               2            1
classif.rpart             1            2
> plotBMRSummary(bmr)
> plotBMRBoxplots(bmr, ber, style = "violin")
Warning: `fun.y` is deprecated. Use `fun` instead.
Warning: `fun.ymin` is deprecated. Use `fun.min` instead.
Warning: `fun.ymax` is deprecated. Use `fun.max` instead.
Warning in max(data$density) :
  no non-missing arguments to max; returning -Inf
Warning: Computation failed in `stat_ydensity()`:
replacement has 1 row, data has 0
Warning in max(data$density) :
  no non-missing arguments to max; returning -Inf
Warning: Computation failed in `stat_ydensity()`:
replacement has 1 row, data has 0
> plotBMRRanksAsBarChart(bmr, pos = "stack")
> friedmanTestBMR(bmr)

	Friedman rank sum test

data:  acc.test.mean and learner.id and task.id
Friedman chi-squared = 0, df = 1, p-value = 1

> friedmanPostHocTestBMR(bmr, p.value = 0.05)
Loading required package: PMCMRplus
Warning in friedmanPostHocTestBMR(bmr, p.value = 0.05) :
  Cannot reject null hypothesis of overall Friedman test,
             returning overall Friedman test.

	Friedman rank sum test

data:  acc.test.mean and learner.id and task.id
Friedman chi-squared = 0, df = 1, p-value = 1

> 
> 
> 
> cleanEx()

detaching ‘package:PMCMRplus’

> nameEx("calculateConfusionMatrix")
> ### * calculateConfusionMatrix
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: calculateConfusionMatrix
> ### Title: Confusion matrix.
> ### Aliases: calculateConfusionMatrix print.ConfusionMatrix
> 
> ### ** Examples
> 
> # get confusion matrix after simple manual prediction
> allinds = 1:150
> train = sample(allinds, 75)
> test = setdiff(allinds, train)
> mod = train("classif.lda", iris.task, subset = train)
> pred = predict(mod, iris.task, subset = test)
> print(calculateConfusionMatrix(pred))
            predicted
true         setosa versicolor virginica -err.-
  setosa         22          0         0      0
  versicolor      0         29         1      1
  virginica       0          0        23      0
  -err.-          0          0         1      1
> print(calculateConfusionMatrix(pred, sums = TRUE))
           setosa versicolor virginica -err.- -n-
setosa         22          0         0      0  22
versicolor      0         29         1      1  30
virginica       0          0        23      0  23
-err.-          0          0         1      1  NA
-n-            22         29        24     NA  75
> print(calculateConfusionMatrix(pred, relative = TRUE))
Relative confusion matrix (normalized by row/column):
            predicted
true         setosa    versicolor virginica -err.-   
  setosa     1.00/1.00 0.00/0.00  0.00/0.00 0.00     
  versicolor 0.00/0.00 0.97/1.00  0.03/0.04 0.03     
  virginica  0.00/0.00 0.00/0.00  1.00/0.96 0.00     
  -err.-          0.00      0.00       0.04 0.01     


Absolute confusion matrix:
            predicted
true         setosa versicolor virginica -err.-
  setosa         22          0         0      0
  versicolor      0         29         1      1
  virginica       0          0        23      0
  -err.-          0          0         1      1
> 
> # now after cross-validation
> r = crossval("classif.lda", iris.task, iters = 2L)
Resampling: cross-validation
Measures:             mmce      
[Resample] iter 1:    0.0266667 
[Resample] iter 2:    0.0133333 


Aggregated Result: mmce.test.mean=0.0200000


> print(calculateConfusionMatrix(r$pred))
            predicted
true         setosa versicolor virginica -err.-
  setosa         50          0         0      0
  versicolor      0         48         2      2
  virginica       0          1        49      1
  -err.-          0          1         2      3
> 
> 
> 
> cleanEx()
> nameEx("calculateROCMeasures")
> ### * calculateROCMeasures
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: calculateROCMeasures
> ### Title: Calculate receiver operator measures.
> ### Aliases: calculateROCMeasures print.ROCMeasures
> 
> ### ** Examples
> 
> lrn = makeLearner("classif.rpart", predict.type = "prob")
> fit = train(lrn, sonar.task)
> pred = predict(fit, task = sonar.task)
> calculateROCMeasures(pred)
    predicted
true M        R                             
   M 95       16        tpr: 0.86 fnr: 0.14 
   R 10       87        fpr: 0.1  tnr: 0.9  
     ppv: 0.9 for: 0.16 lrp: 8.3  acc: 0.88 
     fdr: 0.1 npv: 0.84 lrm: 0.16 dor: 51.66


Abbreviations:
tpr - True positive rate (Sensitivity, Recall)
fpr - False positive rate (Fall-out)
fnr - False negative rate (Miss rate)
tnr - True negative rate (Specificity)
ppv - Positive predictive value (Precision)
for - False omission rate
lrp - Positive likelihood ratio (LR+)
fdr - False discovery rate
npv - Negative predictive value
acc - Accuracy
lrm - Negative likelihood ratio (LR-)
dor - Diagnostic odds ratio
> 
> 
> 
> cleanEx()
> nameEx("capLargeValues")
> ### * capLargeValues
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: capLargeValues
> ### Title: Convert large/infinite numeric values in a data.frame or task.
> ### Aliases: capLargeValues
> 
> ### ** Examples
> 
> capLargeValues(iris, threshold = 5, impute = 5)
    Sepal.Length Sepal.Width Petal.Length Petal.Width    Species
1            5.0         3.5          1.4         0.2     setosa
2            4.9         3.0          1.4         0.2     setosa
3            4.7         3.2          1.3         0.2     setosa
4            4.6         3.1          1.5         0.2     setosa
5            5.0         3.6          1.4         0.2     setosa
6            5.0         3.9          1.7         0.4     setosa
7            4.6         3.4          1.4         0.3     setosa
8            5.0         3.4          1.5         0.2     setosa
9            4.4         2.9          1.4         0.2     setosa
10           4.9         3.1          1.5         0.1     setosa
11           5.0         3.7          1.5         0.2     setosa
12           4.8         3.4          1.6         0.2     setosa
13           4.8         3.0          1.4         0.1     setosa
14           4.3         3.0          1.1         0.1     setosa
15           5.0         4.0          1.2         0.2     setosa
16           5.0         4.4          1.5         0.4     setosa
17           5.0         3.9          1.3         0.4     setosa
18           5.0         3.5          1.4         0.3     setosa
19           5.0         3.8          1.7         0.3     setosa
20           5.0         3.8          1.5         0.3     setosa
21           5.0         3.4          1.7         0.2     setosa
22           5.0         3.7          1.5         0.4     setosa
23           4.6         3.6          1.0         0.2     setosa
24           5.0         3.3          1.7         0.5     setosa
25           4.8         3.4          1.9         0.2     setosa
26           5.0         3.0          1.6         0.2     setosa
27           5.0         3.4          1.6         0.4     setosa
28           5.0         3.5          1.5         0.2     setosa
29           5.0         3.4          1.4         0.2     setosa
30           4.7         3.2          1.6         0.2     setosa
31           4.8         3.1          1.6         0.2     setosa
32           5.0         3.4          1.5         0.4     setosa
33           5.0         4.1          1.5         0.1     setosa
34           5.0         4.2          1.4         0.2     setosa
35           4.9         3.1          1.5         0.2     setosa
36           5.0         3.2          1.2         0.2     setosa
37           5.0         3.5          1.3         0.2     setosa
38           4.9         3.6          1.4         0.1     setosa
39           4.4         3.0          1.3         0.2     setosa
40           5.0         3.4          1.5         0.2     setosa
41           5.0         3.5          1.3         0.3     setosa
42           4.5         2.3          1.3         0.3     setosa
43           4.4         3.2          1.3         0.2     setosa
44           5.0         3.5          1.6         0.6     setosa
45           5.0         3.8          1.9         0.4     setosa
46           4.8         3.0          1.4         0.3     setosa
47           5.0         3.8          1.6         0.2     setosa
48           4.6         3.2          1.4         0.2     setosa
49           5.0         3.7          1.5         0.2     setosa
50           5.0         3.3          1.4         0.2     setosa
51           5.0         3.2          4.7         1.4 versicolor
52           5.0         3.2          4.5         1.5 versicolor
53           5.0         3.1          4.9         1.5 versicolor
54           5.0         2.3          4.0         1.3 versicolor
55           5.0         2.8          4.6         1.5 versicolor
56           5.0         2.8          4.5         1.3 versicolor
57           5.0         3.3          4.7         1.6 versicolor
58           4.9         2.4          3.3         1.0 versicolor
59           5.0         2.9          4.6         1.3 versicolor
60           5.0         2.7          3.9         1.4 versicolor
61           5.0         2.0          3.5         1.0 versicolor
62           5.0         3.0          4.2         1.5 versicolor
63           5.0         2.2          4.0         1.0 versicolor
64           5.0         2.9          4.7         1.4 versicolor
65           5.0         2.9          3.6         1.3 versicolor
66           5.0         3.1          4.4         1.4 versicolor
67           5.0         3.0          4.5         1.5 versicolor
68           5.0         2.7          4.1         1.0 versicolor
69           5.0         2.2          4.5         1.5 versicolor
70           5.0         2.5          3.9         1.1 versicolor
71           5.0         3.2          4.8         1.8 versicolor
72           5.0         2.8          4.0         1.3 versicolor
73           5.0         2.5          4.9         1.5 versicolor
74           5.0         2.8          4.7         1.2 versicolor
75           5.0         2.9          4.3         1.3 versicolor
76           5.0         3.0          4.4         1.4 versicolor
77           5.0         2.8          4.8         1.4 versicolor
78           5.0         3.0          5.0         1.7 versicolor
79           5.0         2.9          4.5         1.5 versicolor
80           5.0         2.6          3.5         1.0 versicolor
81           5.0         2.4          3.8         1.1 versicolor
82           5.0         2.4          3.7         1.0 versicolor
83           5.0         2.7          3.9         1.2 versicolor
84           5.0         2.7          5.0         1.6 versicolor
85           5.0         3.0          4.5         1.5 versicolor
86           5.0         3.4          4.5         1.6 versicolor
87           5.0         3.1          4.7         1.5 versicolor
88           5.0         2.3          4.4         1.3 versicolor
89           5.0         3.0          4.1         1.3 versicolor
90           5.0         2.5          4.0         1.3 versicolor
91           5.0         2.6          4.4         1.2 versicolor
92           5.0         3.0          4.6         1.4 versicolor
93           5.0         2.6          4.0         1.2 versicolor
94           5.0         2.3          3.3         1.0 versicolor
95           5.0         2.7          4.2         1.3 versicolor
96           5.0         3.0          4.2         1.2 versicolor
97           5.0         2.9          4.2         1.3 versicolor
98           5.0         2.9          4.3         1.3 versicolor
99           5.0         2.5          3.0         1.1 versicolor
100          5.0         2.8          4.1         1.3 versicolor
101          5.0         3.3          5.0         2.5  virginica
102          5.0         2.7          5.0         1.9  virginica
103          5.0         3.0          5.0         2.1  virginica
104          5.0         2.9          5.0         1.8  virginica
105          5.0         3.0          5.0         2.2  virginica
106          5.0         3.0          5.0         2.1  virginica
107          4.9         2.5          4.5         1.7  virginica
108          5.0         2.9          5.0         1.8  virginica
109          5.0         2.5          5.0         1.8  virginica
110          5.0         3.6          5.0         2.5  virginica
111          5.0         3.2          5.0         2.0  virginica
112          5.0         2.7          5.0         1.9  virginica
113          5.0         3.0          5.0         2.1  virginica
114          5.0         2.5          5.0         2.0  virginica
115          5.0         2.8          5.0         2.4  virginica
116          5.0         3.2          5.0         2.3  virginica
117          5.0         3.0          5.0         1.8  virginica
118          5.0         3.8          5.0         2.2  virginica
119          5.0         2.6          5.0         2.3  virginica
120          5.0         2.2          5.0         1.5  virginica
121          5.0         3.2          5.0         2.3  virginica
122          5.0         2.8          4.9         2.0  virginica
123          5.0         2.8          5.0         2.0  virginica
124          5.0         2.7          4.9         1.8  virginica
125          5.0         3.3          5.0         2.1  virginica
126          5.0         3.2          5.0         1.8  virginica
127          5.0         2.8          4.8         1.8  virginica
128          5.0         3.0          4.9         1.8  virginica
129          5.0         2.8          5.0         2.1  virginica
130          5.0         3.0          5.0         1.6  virginica
131          5.0         2.8          5.0         1.9  virginica
132          5.0         3.8          5.0         2.0  virginica
133          5.0         2.8          5.0         2.2  virginica
134          5.0         2.8          5.0         1.5  virginica
135          5.0         2.6          5.0         1.4  virginica
136          5.0         3.0          5.0         2.3  virginica
137          5.0         3.4          5.0         2.4  virginica
138          5.0         3.1          5.0         1.8  virginica
139          5.0         3.0          4.8         1.8  virginica
140          5.0         3.1          5.0         2.1  virginica
141          5.0         3.1          5.0         2.4  virginica
142          5.0         3.1          5.0         2.3  virginica
143          5.0         2.7          5.0         1.9  virginica
144          5.0         3.2          5.0         2.3  virginica
145          5.0         3.3          5.0         2.5  virginica
146          5.0         3.0          5.0         2.3  virginica
147          5.0         2.5          5.0         1.9  virginica
148          5.0         3.0          5.0         2.0  virginica
149          5.0         3.4          5.0         2.3  virginica
150          5.0         3.0          5.0         1.8  virginica
> 
> 
> 
> cleanEx()
> nameEx("convertBMRToRankMatrix")
> ### * convertBMRToRankMatrix
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: convertBMRToRankMatrix
> ### Title: Convert BenchmarkResult to a rank-matrix.
> ### Aliases: convertBMRToRankMatrix
> 
> ### ** Examples
> 
> # see benchmark
> 
> 
> 
> cleanEx()
> nameEx("convertMLBenchObjToTask")
> ### * convertMLBenchObjToTask
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: convertMLBenchObjToTask
> ### Title: Convert a machine learning benchmark / demo object from package
> ###   mlbench to a task.
> ### Aliases: convertMLBenchObjToTask
> 
> ### ** Examples
> 
> print(convertMLBenchObjToTask("Ionosphere"))
Loading required package: mlbench
Supervised task: Ionosphere
Type: classif
Target: Class
Observations: 351
Features:
   numerics     factors     ordered functionals 
         32           2           0           0 
Missings: FALSE
Has weights: FALSE
Has blocking: FALSE
Has coordinates: FALSE
Classes: 2
 bad good 
 126  225 
Positive class: bad
> print(convertMLBenchObjToTask("mlbench.spirals", n = 100, sd = 0.1))
Supervised task: mlbench.spirals
Type: classif
Target: classes
Observations: 100
Features:
   numerics     factors     ordered functionals 
          2           0           0           0 
Missings: FALSE
Has weights: FALSE
Has blocking: FALSE
Has coordinates: FALSE
Classes: 2
 1  2 
50 50 
Positive class: 1
> 
> 
> 
> cleanEx()

detaching ‘package:mlbench’

> nameEx("createSpatialResamplingPlots")
> ### * createSpatialResamplingPlots
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: createSpatialResamplingPlots
> ### Title: Create (spatial) resampling plot objects.
> ### Aliases: createSpatialResamplingPlots
> 
> ### ** Examples
> 
> 
> 
> 
> cleanEx()
> nameEx("estimateRelativeOverfitting")
> ### * estimateRelativeOverfitting
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: estimateRelativeOverfitting
> ### Title: Estimate relative overfitting.
> ### Aliases: estimateRelativeOverfitting
> 
> ### ** Examples
> 
> task = makeClassifTask(data = iris, target = "Species")
> rdesc = makeResampleDesc("CV", iters = 2)
> estimateRelativeOverfitting(rdesc, acc, task, makeLearner("classif.knn"))
   iter relative.overfit.acc
1:    1           0.02127660
2:    2          -0.02173913
> estimateRelativeOverfitting(rdesc, acc, task, makeLearner("classif.lda"))
   iter relative.overfit.acc
1:    1          -0.04166667
2:    2           0.04000000
> rpred = resample("classif.knn", task, rdesc)$pred
Resampling: cross-validation
Measures:             mmce      
[Resample] iter 1:    0.0666667 
[Resample] iter 2:    0.0266667 


Aggregated Result: mmce.test.mean=0.0466667


> estimateRelativeOverfitting(rpred, acc, task)
   iter relative.overfit.acc
1:    1           0.06250000
2:    2          -0.06666667
> 
> 
> 
> cleanEx()
> nameEx("extractFDAFeatures")
> ### * extractFDAFeatures
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: extractFDAFeatures
> ### Title: Extract features from functional data.
> ### Aliases: extractFDAFeatures
> 
> ### ** Examples
> 
> df = data.frame(x = matrix(rnorm(24), ncol = 8), y = factor(c("a", "a", "b")))
> fdf = makeFunctionalData(df, fd.features = list(x1 = 1:4, x2 = 5:8), exclude.cols = "y")
> task = makeClassifTask(data = fdf, target = "y")
> extracted = extractFDAFeatures(task,
+   feat.methods = list("x1" = extractFDAFourier(), "x2" = extractFDAWavelets(filter = "haar")))
> print(extracted$task)
Supervised task: fdf
Type: classif
Target: y
Observations: 3
Features:
   numerics     factors     ordered functionals 
          8           0           0           0 
Missings: FALSE
Has weights: FALSE
Has blocking: FALSE
Has coordinates: FALSE
Classes: 2
a b 
2 1 
Positive class: a
> reextractFDAFeatures(task, extracted$desc)
Supervised task: fdf
Type: classif
Target: y
Observations: 3
Features:
   numerics     factors     ordered functionals 
          8           0           0           0 
Missings: FALSE
Has weights: FALSE
Has blocking: FALSE
Has coordinates: FALSE
Classes: 2
a b 
2 1 
Positive class: a
> 
> 
> 
> cleanEx()
> nameEx("filterFeatures")
> ### * filterFeatures
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: filterFeatures
> ### Title: Filter features by thresholding filter values.
> ### Aliases: filterFeatures
> 
> ### ** Examples
> 
> # simple filter
> filterFeatures(iris.task, method = "FSelectorRcpp_gain.ratio", abs = 2)
Supervised task: iris-example
Type: classif
Target: Species
Observations: 150
Features:
   numerics     factors     ordered functionals 
          2           0           0           0 
Missings: FALSE
Has weights: FALSE
Has blocking: FALSE
Has coordinates: FALSE
Classes: 3
    setosa versicolor  virginica 
        50         50         50 
Positive class: NA
> # ensemble filter
> filterFeatures(iris.task, method = "E-min",
+   base.methods = c("FSelectorRcpp_gain.ratio",
+     "FSelectorRcpp_information.gain"), abs = 2)
Supervised task: iris-example
Type: classif
Target: Species
Observations: 150
Features:
   numerics     factors     ordered functionals 
          2           0           0           0 
Missings: FALSE
Has weights: FALSE
Has blocking: FALSE
Has coordinates: FALSE
Classes: 3
    setosa versicolor  virginica 
        50         50         50 
Positive class: NA
> 
> 
> 
> cleanEx()
> nameEx("friedmanPostHocTestBMR")
> ### * friedmanPostHocTestBMR
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: friedmanPostHocTestBMR
> ### Title: Perform a posthoc Friedman-Nemenyi test.
> ### Aliases: friedmanPostHocTestBMR
> 
> ### ** Examples
> 
> # see benchmark
> 
> 
> 
> cleanEx()
> nameEx("friedmanTestBMR")
> ### * friedmanTestBMR
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: friedmanTestBMR
> ### Title: Perform overall Friedman test for a BenchmarkResult.
> ### Aliases: friedmanTestBMR
> 
> ### ** Examples
> 
> # see benchmark
> 
> 
> 
> cleanEx()
> nameEx("generateFeatureImportanceData")
> ### * generateFeatureImportanceData
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: generateFeatureImportanceData
> ### Title: Generate feature importance.
> ### Aliases: generateFeatureImportanceData FeatureImportanceData
> 
> ### ** Examples
> 
> 
> lrn = makeLearner("classif.rpart", predict.type = "prob")
> fit = train(lrn, iris.task)
> imp = generateFeatureImportanceData(iris.task, "permutation.importance",
+   lrn, "Petal.Width", nmc = 10L, local = TRUE)
> 
> 
> 
> cleanEx()
> nameEx("generateFilterValuesData")
> ### * generateFilterValuesData
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: generateFilterValuesData
> ### Title: Calculates feature filter values.
> ### Aliases: generateFilterValuesData FilterValues
> 
> ### ** Examples
> 
> # two simple filter methods
> fval = generateFilterValuesData(iris.task,
+   method = c("FSelectorRcpp_gain.ratio", "FSelectorRcpp_information.gain"))
> # using ensemble method "E-mean"
> fval = generateFilterValuesData(iris.task,
+   method = list("E-mean", c("FSelectorRcpp_gain.ratio",
+     "FSelectorRcpp_information.gain")))
> 
> 
> 
> cleanEx()
> nameEx("generateHyperParsEffectData")
> ### * generateHyperParsEffectData
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: generateHyperParsEffectData
> ### Title: Generate hyperparameter effect data.
> ### Aliases: generateHyperParsEffectData
> 
> ### ** Examples
> 
> ## Not run: 
> ##D # 3-fold cross validation
> ##D ps = makeParamSet(makeDiscreteParam("C", values = 2^(-4:4)))
> ##D ctrl = makeTuneControlGrid()
> ##D rdesc = makeResampleDesc("CV", iters = 3L)
> ##D res = tuneParams("classif.ksvm", task = pid.task, resampling = rdesc,
> ##D   par.set = ps, control = ctrl)
> ##D data = generateHyperParsEffectData(res)
> ##D plt = plotHyperParsEffect(data, x = "C", y = "mmce.test.mean")
> ##D plt + ylab("Misclassification Error")
> ##D 
> ##D # nested cross validation
> ##D ps = makeParamSet(makeDiscreteParam("C", values = 2^(-4:4)))
> ##D ctrl = makeTuneControlGrid()
> ##D rdesc = makeResampleDesc("CV", iters = 3L)
> ##D lrn = makeTuneWrapper("classif.ksvm", control = ctrl,
> ##D   resampling = rdesc, par.set = ps)
> ##D res = resample(lrn, task = pid.task, resampling = cv2,
> ##D   extract = getTuneResult)
> ##D data = generateHyperParsEffectData(res)
> ##D plotHyperParsEffect(data, x = "C", y = "mmce.test.mean", plot.type = "line")
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("generateLearningCurveData")
> ### * generateLearningCurveData
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: generateLearningCurveData
> ### Title: Generates a learning curve.
> ### Aliases: generateLearningCurveData LearningCurveData
> 
> ### ** Examples
> 
> r = generateLearningCurveData(list("classif.rpart", "classif.knn"),
+   task = sonar.task, percs = seq(0.2, 1, by = 0.2),
+   measures = list(tp, fp, tn, fn),
+   resampling = makeResampleDesc(method = "Subsample", iters = 5),
+   show.info = FALSE)
> plotLearningCurve(r)
> 
> 
> 
> cleanEx()
> nameEx("generatePartialDependenceData")
> ### * generatePartialDependenceData
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: generatePartialDependenceData
> ### Title: Generate partial dependence.
> ### Aliases: generatePartialDependenceData PartialDependenceData
> 
> ### ** Examples
> 
> lrn = makeLearner("regr.svm")
> fit = train(lrn, bh.task)
> pd = generatePartialDependenceData(fit, bh.task, "lstat")
Loading required package: mmpf
> plotPartialDependence(pd, data = getTaskData(bh.task))
> 
> lrn = makeLearner("classif.rpart", predict.type = "prob")
> fit = train(lrn, iris.task)
> pd = generatePartialDependenceData(fit, iris.task, "Petal.Width")
> plotPartialDependence(pd, data = getTaskData(iris.task))
> 
> 
> 
> cleanEx()

detaching ‘package:mmpf’

> nameEx("getCaretParamSet")
> ### * getCaretParamSet
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: getCaretParamSet
> ### Title: Get tuning parameters from a learner of the caret R-package.
> ### Aliases: getCaretParamSet
> 
> ### ** Examples
> 
> if (requireNamespace("caret") && requireNamespace("mlbench")) {
+   library(caret)
+   classifTask = makeClassifTask(data = iris, target = "Species")
+ 
+   # (1) classification (random forest) with discretized parameters
+   getCaretParamSet("rf", length = 9L, task = classifTask, discretize = TRUE)
+ 
+   # (2) regression (gradient boosting machine) without discretized parameters
+   library(mlbench)
+   data(BostonHousing)
+   regrTask = makeRegrTask(data = BostonHousing, target = "medv")
+   getCaretParamSet("gbm", length = 9L, task = regrTask, discretize = FALSE)
+ }
Loading required namespace: caret
Loading required package: lattice
Loading required package: ggplot2

Attaching package: ‘caret’

The following object is masked from ‘package:mlr’:

    train

note: only 3 unique complexity parameters in default grid. Truncating the grid to 3 .

$par.vals
$par.vals$shrinkage
[1] 0.1

$par.vals$n.minobsinnode
[1] 10


$par.set
                     Type len Def    Constr Req Tunable Trafo
interaction.depth integer   -   -    1 to 9   -    TRUE     -
n.trees           numeric   -   - 50 to 450   -    TRUE     -

> 
> 
> 
> cleanEx()

detaching ‘package:mlbench’, ‘package:caret’, ‘package:ggplot2’,
  ‘package:lattice’

> nameEx("getHyperPars")
> ### * getHyperPars
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: getHyperPars
> ### Title: Get current parameter settings for a learner.
> ### Aliases: getHyperPars
> 
> ### ** Examples
> 
> getHyperPars(makeLearner("classif.ranger"))
$num.threads
[1] 1

$verbose
[1] FALSE

$respect.unordered.factors
[1] "order"

> 
> ## set learner hyperparameter `mtry` manually
> getHyperPars(makeLearner("classif.ranger", mtry = 100))
$num.threads
[1] 1

$verbose
[1] FALSE

$respect.unordered.factors
[1] "order"

$mtry
[1] 100

> 
> 
> 
> cleanEx()
> nameEx("getMultilabelBinaryPerformances")
> ### * getMultilabelBinaryPerformances
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: getMultilabelBinaryPerformances
> ### Title: Retrieve binary classification measures for multilabel
> ###   classification predictions.
> ### Aliases: getMultilabelBinaryPerformances
> 
> ### ** Examples
> 
> # see makeMultilabelBinaryRelevanceWrapper
> 
> 
> 
> cleanEx()
> nameEx("getNestedTuneResultsOptPathDf")
> ### * getNestedTuneResultsOptPathDf
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: getNestedTuneResultsOptPathDf
> ### Title: Get the 'opt.path's from each tuning step from the outer
> ###   resampling.
> ### Aliases: getNestedTuneResultsOptPathDf
> 
> ### ** Examples
> 
> # see example of makeTuneWrapper
> 
> 
> 
> cleanEx()
> nameEx("getNestedTuneResultsX")
> ### * getNestedTuneResultsX
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: getNestedTuneResultsX
> ### Title: Get the tuned hyperparameter settings from a nested tuning.
> ### Aliases: getNestedTuneResultsX
> 
> ### ** Examples
> 
> # see example of makeTuneWrapper
> 
> 
> 
> cleanEx()
> nameEx("getOOBPreds")
> ### * getOOBPreds
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: getOOBPreds
> ### Title: Extracts out-of-bag predictions from trained models.
> ### Aliases: getOOBPreds
> 
> ### ** Examples
> 
> training.set = sample(1:150, 50)
> lrn = makeLearner("classif.ranger", predict.type = "prob", predict.threshold = 0.6)
> mod = train(lrn, sonar.task, subset = training.set)
> oob = getOOBPreds(mod, sonar.task)
> oob
Prediction: 50 observations
predict.type: prob
threshold: M=0.60,R=0.40
time: NA
     id truth     prob.M    prob.R response
68   68     R 0.11643991 0.8835601        R
129 129     M 0.49663407 0.5033659        R
43   43     R 0.03353057 0.9664694        R
14   14     R 0.23703704 0.7629630        R
51   51     R 0.18450627 0.8154937        R
85   85     R 0.42724238 0.5727576        R
... (#rows: 50, #cols: 5)
> performance(oob, measures = list(auc, mmce))
      auc      mmce 
0.8841912 0.3200000 
> 
> 
> 
> cleanEx()
> nameEx("getPredictionProbabilities")
> ### * getPredictionProbabilities
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: getPredictionProbabilities
> ### Title: Get probabilities for some classes.
> ### Aliases: getPredictionProbabilities
> 
> ### ** Examples
> 
> task = makeClassifTask(data = iris, target = "Species")
> lrn = makeLearner("classif.lda", predict.type = "prob")
> mod = train(lrn, task)
> # predict probabilities
> pred = predict(mod, newdata = iris)
> 
> # Get probabilities for all classes
> head(getPredictionProbabilities(pred))
  setosa   versicolor    virginica
1      1 3.896358e-22 2.611168e-42
2      1 7.217970e-18 5.042143e-37
3      1 1.463849e-19 4.675932e-39
4      1 1.268536e-16 3.566610e-35
5      1 1.637387e-22 1.082605e-42
6      1 3.883282e-21 4.566540e-40
> 
> # Get probabilities for a subset of classes
> head(getPredictionProbabilities(pred, c("setosa", "virginica")))
  setosa    virginica
1      1 2.611168e-42
2      1 5.042143e-37
3      1 4.675932e-39
4      1 3.566610e-35
5      1 1.082605e-42
6      1 4.566540e-40
> 
> 
> 
> cleanEx()
> nameEx("getResamplingIndices")
> ### * getResamplingIndices
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: getResamplingIndices
> ### Title: Get the resampling indices from a tuning or feature selection
> ###   wrapper..
> ### Aliases: getResamplingIndices
> 
> ### ** Examples
> 
> task = makeClassifTask(data = iris, target = "Species")
> lrn = makeLearner("classif.rpart")
> # stupid mini grid
> ps = makeParamSet(
+   makeDiscreteParam("cp", values = c(0.05, 0.1)),
+   makeDiscreteParam("minsplit", values = c(10, 20))
+ )
> ctrl = makeTuneControlGrid()
> inner = makeResampleDesc("Holdout")
> outer = makeResampleDesc("CV", iters = 2)
> lrn = makeTuneWrapper(lrn, resampling = inner, par.set = ps, control = ctrl)
> # nested resampling for evaluation
> # we also extract tuned hyper pars in each iteration and by that the resampling indices
> r = resample(lrn, task, outer, extract = getTuneResult)
Resampling: cross-validation
Measures:             mmce      
[Tune] Started tuning learner classif.rpart for parameter set:
             Type len Def   Constr Req Tunable Trafo
cp       discrete   -   - 0.05,0.1   -    TRUE     -
minsplit discrete   -   -    10,20   -    TRUE     -
With control class: TuneControlGrid
Imputation value: 1
[Tune-x] 1: cp=0.05; minsplit=10
[Tune-y] 1: mmce.test.mean=0.1200000; time: 0.0 min
[Tune-x] 2: cp=0.1; minsplit=10
[Tune-y] 2: mmce.test.mean=0.1200000; time: 0.0 min
[Tune-x] 3: cp=0.05; minsplit=20
[Tune-y] 3: mmce.test.mean=0.1200000; time: 0.0 min
[Tune-x] 4: cp=0.1; minsplit=20
[Tune-y] 4: mmce.test.mean=0.1200000; time: 0.0 min
[Tune] Result: cp=0.05; minsplit=10 : mmce.test.mean=0.1200000
[Resample] iter 1:    0.0666667 
[Tune] Started tuning learner classif.rpart for parameter set:
             Type len Def   Constr Req Tunable Trafo
cp       discrete   -   - 0.05,0.1   -    TRUE     -
minsplit discrete   -   -    10,20   -    TRUE     -
With control class: TuneControlGrid
Imputation value: 1
[Tune-x] 1: cp=0.05; minsplit=10
[Tune-y] 1: mmce.test.mean=0.0800000; time: 0.0 min
[Tune-x] 2: cp=0.1; minsplit=10
[Tune-y] 2: mmce.test.mean=0.0800000; time: 0.0 min
[Tune-x] 3: cp=0.05; minsplit=20
[Tune-y] 3: mmce.test.mean=0.0800000; time: 0.0 min
[Tune-x] 4: cp=0.1; minsplit=20
[Tune-y] 4: mmce.test.mean=0.0800000; time: 0.0 min
[Tune] Result: cp=0.05; minsplit=20 : mmce.test.mean=0.0800000
[Resample] iter 2:    0.1600000 


Aggregated Result: mmce.test.mean=0.1133333


> # get tuning indices
> getResamplingIndices(r, inner = TRUE)
[[1]]
[[1]]$train.inds
[[1]]$train.inds[[1]]
 [1]   8 125  91 100  65  97 135  96  67 102  58  16  33  79  41  24  70  42  48
[20]  73   4  92  23 120 110 141  47  40  76 121   9  75  93  44   1  38 122  28
[39]  25  30 145 139 113  69  86  81  62  59  27 144


[[1]]$test.inds
[[1]]$test.inds[[1]]
 [1]  21 105  18 108  29 136 124  15  53 118  85 150  52 112  87 123 119  22 142
[20]  57  46 128  88  83  49



[[2]]
[[2]]$train.inds
[[2]]$train.inds[[1]]
 [1]  82 117 133   7  17 132  39  72  77 140 147  50 149  64   5 104 126  31  43
[20] 143  13 130  37  74 114  78   6  68 138  60 116  89 109  14  84  20   3 111
[39]  45 134  66 106  56 101 148  12 103  26  35  61


[[2]]$test.inds
[[2]]$test.inds[[1]]
 [1]  90 107  95  36  34  10  54  94 137  32 146  51  80  98 131  71  55  99  11
[20] 129 115   2  63 127  19



> 
> 
> 
> cleanEx()
> nameEx("getTaskData")
> ### * getTaskData
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: getTaskData
> ### Title: Extract data in task.
> ### Aliases: getTaskData
> 
> ### ** Examples
> 
> library("mlbench")
> data(BreastCancer)
> 
> df = BreastCancer
> df$Id = NULL
> task = makeClassifTask(id = "BreastCancer", data = df, target = "Class", positive = "malignant")
> head(getTaskData)
                                                                 
1 function (task, subset = NULL, features, target.extra = FALSE, 
2     recode.target = "no", functionals.as = "dfcols")           
3 {                                                              
4     checkTask(task, "Task")                                    
5     checkTaskSubset(subset, size = task$task.desc$size)        
6     assertLogical(target.extra)                                
> head(getTaskData(task, features = c("Cell.size", "Cell.shape"), recode.target = "-1+1"))
  Cell.size Cell.shape Class
1         1          1    -1
2         4          4    -1
3         1          1    -1
4         8          8    -1
5         1          1    -1
6        10         10     1
> head(getTaskData(task, subset = 1:100, recode.target = "01"))
  Cl.thickness Cell.size Cell.shape Marg.adhesion Epith.c.size Bare.nuclei
1            5         1          1             1            2           1
2            5         4          4             5            7          10
3            3         1          1             1            2           2
4            6         8          8             1            3           4
5            4         1          1             3            2           1
6            8        10         10             8            7          10
  Bl.cromatin Normal.nucleoli Mitoses Class
1           3               1       1     0
2           3               2       1     0
3           3               1       1     0
4           3               7       1     0
5           3               1       1     0
6           9               7       1     1
> 
> 
> 
> cleanEx()

detaching ‘package:mlbench’

> nameEx("getTaskTargets")
> ### * getTaskTargets
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: getTaskTargets
> ### Title: Get target data of task.
> ### Aliases: getTaskTargets
> 
> ### ** Examples
> 
> task = makeClassifTask(data = iris, target = "Species")
> getTaskTargets(task)
  [1] setosa     setosa     setosa     setosa     setosa     setosa    
  [7] setosa     setosa     setosa     setosa     setosa     setosa    
 [13] setosa     setosa     setosa     setosa     setosa     setosa    
 [19] setosa     setosa     setosa     setosa     setosa     setosa    
 [25] setosa     setosa     setosa     setosa     setosa     setosa    
 [31] setosa     setosa     setosa     setosa     setosa     setosa    
 [37] setosa     setosa     setosa     setosa     setosa     setosa    
 [43] setosa     setosa     setosa     setosa     setosa     setosa    
 [49] setosa     setosa     versicolor versicolor versicolor versicolor
 [55] versicolor versicolor versicolor versicolor versicolor versicolor
 [61] versicolor versicolor versicolor versicolor versicolor versicolor
 [67] versicolor versicolor versicolor versicolor versicolor versicolor
 [73] versicolor versicolor versicolor versicolor versicolor versicolor
 [79] versicolor versicolor versicolor versicolor versicolor versicolor
 [85] versicolor versicolor versicolor versicolor versicolor versicolor
 [91] versicolor versicolor versicolor versicolor versicolor versicolor
 [97] versicolor versicolor versicolor versicolor virginica  virginica 
[103] virginica  virginica  virginica  virginica  virginica  virginica 
[109] virginica  virginica  virginica  virginica  virginica  virginica 
[115] virginica  virginica  virginica  virginica  virginica  virginica 
[121] virginica  virginica  virginica  virginica  virginica  virginica 
[127] virginica  virginica  virginica  virginica  virginica  virginica 
[133] virginica  virginica  virginica  virginica  virginica  virginica 
[139] virginica  virginica  virginica  virginica  virginica  virginica 
[145] virginica  virginica  virginica  virginica  virginica  virginica 
Levels: setosa versicolor virginica
> 
> 
> 
> cleanEx()
> nameEx("impute")
> ### * impute
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: impute
> ### Title: Impute and re-impute data
> ### Aliases: impute
> 
> ### ** Examples
> 
> df = data.frame(x = c(1, 1, NA), y = factor(c("a", "a", "b")), z = 1:3)
> imputed = impute(df, target = character(0), cols = list(x = 99, y = imputeMode()))
> print(imputed$data)
   x y z
1  1 a 1
2  1 a 2
3 99 b 3
> reimpute(data.frame(x = NA_real_), imputed$desc)
   x y  z
1 99 a NA
> 
> 
> 
> cleanEx()
> nameEx("joinClassLevels")
> ### * joinClassLevels
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: joinClassLevels
> ### Title: Join some class existing levels to new, larger class levels for
> ###   classification problems.
> ### Aliases: joinClassLevels
> 
> ### ** Examples
> 
> joinClassLevels(iris.task, new.levels = list(foo = c("setosa", "virginica")))
Supervised task: iris-example
Type: classif
Target: Species
Observations: 150
Features:
   numerics     factors     ordered functionals 
          4           0           0           0 
Missings: FALSE
Has weights: FALSE
Has blocking: FALSE
Has coordinates: FALSE
Classes: 2
       foo versicolor 
       100         50 
Positive class: foo
> 
> 
> 
> cleanEx()
> nameEx("listLearners")
> ### * listLearners
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: listLearners
> ### Title: Find matching learning algorithms.
> ### Aliases: listLearners listLearners.default listLearners.character
> ###   listLearners.Task
> 
> ### ** Examples
> 
> ## Not run: 
> ##D listLearners("classif", properties = c("multiclass", "prob"))
> ##D data = iris
> ##D task = makeClassifTask(data = data, target = "Species")
> ##D listLearners(task)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("makeAggregation")
> ### * makeAggregation
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: makeAggregation
> ### Title: Specify your own aggregation of measures.
> ### Aliases: makeAggregation
> 
> ### ** Examples
> 
> # computes the interquartile range on all performance values
> test.iqr = makeAggregation(
+   id = "test.iqr", name = "Test set interquartile range",
+   properties = "req.test",
+   fun = function(task, perf.test, perf.train, measure, group, pred) IQR(perf.test)
+ )
> 
> 
> 
> cleanEx()
> nameEx("makeClassificationViaRegressionWrapper")
> ### * makeClassificationViaRegressionWrapper
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: makeClassificationViaRegressionWrapper
> ### Title: Classification via regression wrapper.
> ### Aliases: makeClassificationViaRegressionWrapper
> 
> ### ** Examples
> 
> lrn = makeLearner("regr.rpart")
> lrn = makeClassificationViaRegressionWrapper(lrn)
> mod = train(lrn, sonar.task, subset = 1:140)
> predictions = predict(mod, newdata = getTaskData(sonar.task)[141:208, 1:60])
> 
> 
> 
> cleanEx()
> nameEx("makeFeatSelWrapper")
> ### * makeFeatSelWrapper
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: makeFeatSelWrapper
> ### Title: Fuse learner with feature selection.
> ### Aliases: makeFeatSelWrapper
> 
> ### ** Examples
> 
> # nested resampling with feature selection (with a nonsense algorithm for selection)
> outer = makeResampleDesc("CV", iters = 2L)
> inner = makeResampleDesc("Holdout")
> ctrl = makeFeatSelControlRandom(maxit = 1)
> lrn = makeFeatSelWrapper("classif.ksvm", resampling = inner, control = ctrl)
> # we also extract the selected features for all iteration here
> r = resample(lrn, iris.task, outer, extract = getFeatSelResult)
Resampling: cross-validation
Measures:             mmce      
[FeatSel] Started selecting features for learner 'classif.ksvm'
With control class: FeatSelControlRandom
Imputation value: 1
[FeatSel-x] 1: 0010 (1 bits)
[FeatSel-y] 1: mmce.test.mean=0.0800000; time: 0.1 min
Warning in sprintf(...) :
  one argument not used by format '[FeatSel] Result: %s (%i bits)'
[FeatSel] Result: Petal.Length (1 bits)
[Resample] iter 1:    0.0666667 
[FeatSel] Started selecting features for learner 'classif.ksvm'
With control class: FeatSelControlRandom
Imputation value: 1
[FeatSel-x] 1: 1111 (4 bits)
[FeatSel-y] 1: mmce.test.mean=0.0800000; time: 0.0 min
Warning in sprintf(...) :
  one argument not used by format '[FeatSel] Result: %s (%i bits)'
[FeatSel] Result: Sepal.Length,Sepal.Width,Pe... (4 bits)
[Resample] iter 2:    0.0266667 


Aggregated Result: mmce.test.mean=0.0466667


> 
> 
> 
> cleanEx()
> nameEx("makeFilterWrapper")
> ### * makeFilterWrapper
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: makeFilterWrapper
> ### Title: Fuse learner with a feature filter method.
> ### Aliases: makeFilterWrapper
> 
> ### ** Examples
> 
> 
> 
> 
> cleanEx()
> nameEx("makeFunctionalData")
> ### * makeFunctionalData
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: makeFunctionalData
> ### Title: Create a data.frame containing functional features from a normal
> ###   data.frame.
> ### Aliases: makeFunctionalData
> 
> ### ** Examples
> 
> # data.frame where columns 1:6 and 8:10 belong to a functional feature
> d1 = data.frame(matrix(rnorm(100), nrow = 10), "target" = seq_len(10))
> # Transform to functional data
> d2 = makeFunctionalData(d1, fd.features = list("fd1" = 1:6, "fd2" = 8:10))
> # Create a regression task
> makeRegrTask(data = d2, target = "target")
Supervised task: d2
Type: regr
Target: target
Observations: 10
Features:
   numerics     factors     ordered functionals 
          1           0           0           2 
Missings: FALSE
Has weights: FALSE
Has blocking: FALSE
Has coordinates: FALSE
> 
> 
> 
> cleanEx()
> nameEx("makeLearner")
> ### * makeLearner
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: makeLearner
> ### Title: Create learner object.
> ### Aliases: makeLearner Learner
> 
> ### ** Examples
> 
> makeLearner("classif.rpart")
Learner classif.rpart from package rpart
Type: classif
Name: Decision Tree; Short name: rpart
Class: classif.rpart
Properties: twoclass,multiclass,missings,numerics,factors,ordered,prob,weights,featimp
Predict-Type: response
Hyperparameters: xval=0

> makeLearner("classif.lda", predict.type = "prob")
Learner classif.lda from package MASS
Type: classif
Name: Linear Discriminant Analysis; Short name: lda
Class: classif.lda
Properties: twoclass,multiclass,numerics,factors,prob
Predict-Type: prob
Hyperparameters: 

> lrn = makeLearner("classif.lda", method = "t", nu = 10)
> getHyperPars(lrn)
$method
[1] "t"

$nu
[1] 10

> 
> 
> 
> cleanEx()
> nameEx("makeLearners")
> ### * makeLearners
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: makeLearners
> ### Title: Create multiple learners at once.
> ### Aliases: makeLearners
> 
> ### ** Examples
> 
> makeLearners(c("rpart", "lda"), type = "classif", predict.type = "prob")
$classif.rpart
Learner classif.rpart from package rpart
Type: classif
Name: Decision Tree; Short name: rpart
Class: classif.rpart
Properties: twoclass,multiclass,missings,numerics,factors,ordered,prob,weights,featimp
Predict-Type: prob
Hyperparameters: xval=0


$classif.lda
Learner classif.lda from package MASS
Type: classif
Name: Linear Discriminant Analysis; Short name: lda
Class: classif.lda
Properties: twoclass,multiclass,numerics,factors,prob
Predict-Type: prob
Hyperparameters: 


> 
> 
> 
> cleanEx()
> nameEx("makeMeasure")
> ### * makeMeasure
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: makeMeasure
> ### Title: Construct performance measure.
> ### Aliases: makeMeasure Measure
> 
> ### ** Examples
> 
> f = function(task, model, pred, extra.args) {
+   sum((pred$data$response - pred$data$truth)^2)
+ }
> makeMeasure(id = "my.sse", minimize = TRUE,
+   properties = c("regr", "response"), fun = f)
Name: my.sse
Performance measure: my.sse
Properties: regr,response
Minimize: TRUE
Best: -Inf; Worst: Inf
Aggregated by: test.mean
Arguments: 
Note: 
> 
> 
> 
> cleanEx()
> nameEx("makeModelMultiplexer")
> ### * makeModelMultiplexer
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: makeModelMultiplexer
> ### Title: Create model multiplexer for model selection to tune over
> ###   multiple possible models.
> ### Aliases: makeModelMultiplexer ModelMultiplexer
> 
> ### ** Examples
> 
> set.seed(123)
> 
> 
> 
> cleanEx()
> nameEx("makeModelMultiplexerParamSet")
> ### * makeModelMultiplexerParamSet
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: makeModelMultiplexerParamSet
> ### Title: Creates a parameter set for model multiplexer tuning.
> ### Aliases: makeModelMultiplexerParamSet
> 
> ### ** Examples
> 
> # See makeModelMultiplexer
> 
> 
> 
> cleanEx()
> nameEx("makeMultilabelBinaryRelevanceWrapper")
> ### * makeMultilabelBinaryRelevanceWrapper
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: makeMultilabelBinaryRelevanceWrapper
> ### Title: Use binary relevance method to create a multilabel learner.
> ### Aliases: makeMultilabelBinaryRelevanceWrapper
> 
> ### ** Examples
> 
> d = getTaskData(yeast.task)
> # drop some labels so example runs faster
> d = d[seq(1, nrow(d), by = 20), c(1:2, 15:17)]
> task = makeMultilabelTask(data = d, target = c("label1", "label2"))
> lrn = makeLearner("classif.rpart")
> lrn = makeMultilabelBinaryRelevanceWrapper(lrn)
> lrn = setPredictType(lrn, "prob")
> # train, predict and evaluate
> mod = train(lrn, task)
> pred = predict(mod, task)
> performance(pred, measure = list(multilabel.hamloss, multilabel.subset01, multilabel.f1))
 multilabel.hamloss multilabel.subset01       multilabel.f1 
          0.2272727           0.3388430           0.6942149 
> # the next call basically has the same structure for any multilabel meta wrapper
> getMultilabelBinaryPerformances(pred, measures = list(mmce, auc))
       mmce.test.mean auc.test.mean
label1      0.1900826     0.6887052
label2      0.2644628     0.6883117
> # above works also with predictions from resample!
> 
> 
> 
> 
> cleanEx()
> nameEx("makeMultilabelClassifierChainsWrapper")
> ### * makeMultilabelClassifierChainsWrapper
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: makeMultilabelClassifierChainsWrapper
> ### Title: Use classifier chains method (CC) to create a multilabel
> ###   learner.
> ### Aliases: makeMultilabelClassifierChainsWrapper
> 
> ### ** Examples
> 
> d = getTaskData(yeast.task)
> # drop some labels so example runs faster
> d = d[seq(1, nrow(d), by = 20), c(1:2, 15:17)]
> task = makeMultilabelTask(data = d, target = c("label1", "label2"))
> lrn = makeLearner("classif.rpart")
> lrn = makeMultilabelBinaryRelevanceWrapper(lrn)
> lrn = setPredictType(lrn, "prob")
> # train, predict and evaluate
> mod = train(lrn, task)
> pred = predict(mod, task)
> performance(pred, measure = list(multilabel.hamloss, multilabel.subset01, multilabel.f1))
 multilabel.hamloss multilabel.subset01       multilabel.f1 
          0.2272727           0.3388430           0.6942149 
> # the next call basically has the same structure for any multilabel meta wrapper
> getMultilabelBinaryPerformances(pred, measures = list(mmce, auc))
       mmce.test.mean auc.test.mean
label1      0.1900826     0.6887052
label2      0.2644628     0.6883117
> # above works also with predictions from resample!
> 
> 
> 
> 
> cleanEx()
> nameEx("makeMultilabelDBRWrapper")
> ### * makeMultilabelDBRWrapper
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: makeMultilabelDBRWrapper
> ### Title: Use dependent binary relevance method (DBR) to create a
> ###   multilabel learner.
> ### Aliases: makeMultilabelDBRWrapper
> 
> ### ** Examples
> 
> d = getTaskData(yeast.task)
> # drop some labels so example runs faster
> d = d[seq(1, nrow(d), by = 20), c(1:2, 15:17)]
> task = makeMultilabelTask(data = d, target = c("label1", "label2"))
> lrn = makeLearner("classif.rpart")
> lrn = makeMultilabelBinaryRelevanceWrapper(lrn)
> lrn = setPredictType(lrn, "prob")
> # train, predict and evaluate
> mod = train(lrn, task)
> pred = predict(mod, task)
> performance(pred, measure = list(multilabel.hamloss, multilabel.subset01, multilabel.f1))
 multilabel.hamloss multilabel.subset01       multilabel.f1 
          0.2272727           0.3388430           0.6942149 
> # the next call basically has the same structure for any multilabel meta wrapper
> getMultilabelBinaryPerformances(pred, measures = list(mmce, auc))
       mmce.test.mean auc.test.mean
label1      0.1900826     0.6887052
label2      0.2644628     0.6883117
> # above works also with predictions from resample!
> 
> 
> 
> 
> cleanEx()
> nameEx("makeMultilabelNestedStackingWrapper")
> ### * makeMultilabelNestedStackingWrapper
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: makeMultilabelNestedStackingWrapper
> ### Title: Use nested stacking method to create a multilabel learner.
> ### Aliases: makeMultilabelNestedStackingWrapper
> 
> ### ** Examples
> 
> d = getTaskData(yeast.task)
> # drop some labels so example runs faster
> d = d[seq(1, nrow(d), by = 20), c(1:2, 15:17)]
> task = makeMultilabelTask(data = d, target = c("label1", "label2"))
> lrn = makeLearner("classif.rpart")
> lrn = makeMultilabelBinaryRelevanceWrapper(lrn)
> lrn = setPredictType(lrn, "prob")
> # train, predict and evaluate
> mod = train(lrn, task)
> pred = predict(mod, task)
> performance(pred, measure = list(multilabel.hamloss, multilabel.subset01, multilabel.f1))
 multilabel.hamloss multilabel.subset01       multilabel.f1 
          0.2272727           0.3388430           0.6942149 
> # the next call basically has the same structure for any multilabel meta wrapper
> getMultilabelBinaryPerformances(pred, measures = list(mmce, auc))
       mmce.test.mean auc.test.mean
label1      0.1900826     0.6887052
label2      0.2644628     0.6883117
> # above works also with predictions from resample!
> 
> 
> 
> 
> cleanEx()
> nameEx("makeMultilabelStackingWrapper")
> ### * makeMultilabelStackingWrapper
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: makeMultilabelStackingWrapper
> ### Title: Use stacking method (stacked generalization) to create a
> ###   multilabel learner.
> ### Aliases: makeMultilabelStackingWrapper
> 
> ### ** Examples
> 
> d = getTaskData(yeast.task)
> # drop some labels so example runs faster
> d = d[seq(1, nrow(d), by = 20), c(1:2, 15:17)]
> task = makeMultilabelTask(data = d, target = c("label1", "label2"))
> lrn = makeLearner("classif.rpart")
> lrn = makeMultilabelBinaryRelevanceWrapper(lrn)
> lrn = setPredictType(lrn, "prob")
> # train, predict and evaluate
> mod = train(lrn, task)
> pred = predict(mod, task)
> performance(pred, measure = list(multilabel.hamloss, multilabel.subset01, multilabel.f1))
 multilabel.hamloss multilabel.subset01       multilabel.f1 
          0.2272727           0.3388430           0.6942149 
> # the next call basically has the same structure for any multilabel meta wrapper
> getMultilabelBinaryPerformances(pred, measures = list(mmce, auc))
       mmce.test.mean auc.test.mean
label1      0.1900826     0.6887052
label2      0.2644628     0.6883117
> # above works also with predictions from resample!
> 
> 
> 
> 
> cleanEx()
> nameEx("makeResampleDesc")
> ### * makeResampleDesc
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: makeResampleDesc
> ### Title: Create a description object for a resampling strategy.
> ### Aliases: makeResampleDesc ResampleDesc hout cv2 cv3 cv5 cv10
> 
> ### ** Examples
> 
> # Bootstraping
> makeResampleDesc("Bootstrap", iters = 10)
Resample description: OOB bootstrapping with 10 iterations.
Predict: test
Stratification: FALSE
> makeResampleDesc("Bootstrap", iters = 10, predict = "both")
Resample description: OOB bootstrapping with 10 iterations.
Predict: both
Stratification: FALSE
> 
> # Subsampling
> makeResampleDesc("Subsample", iters = 10, split = 3 / 4)
Resample description: subsampling with 10 iterations and 0.75 split rate.
Predict: test
Stratification: FALSE
> makeResampleDesc("Subsample", iters = 10)
Resample description: subsampling with 10 iterations and 0.67 split rate.
Predict: test
Stratification: FALSE
> 
> # Holdout a.k.a. test sample estimation
> makeResampleDesc("Holdout")
Resample description: holdout with 0.67 split rate.
Predict: test
Stratification: FALSE
> 
> 
> 
> cleanEx()
> nameEx("makeResampleInstance")
> ### * makeResampleInstance
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: makeResampleInstance
> ### Title: Instantiates a resampling strategy object.
> ### Aliases: makeResampleInstance ResampleInstance
> 
> ### ** Examples
> 
> rdesc = makeResampleDesc("Bootstrap", iters = 10)
> rin = makeResampleInstance(rdesc, task = iris.task)
> 
> rdesc = makeResampleDesc("CV", iters = 50)
> rin = makeResampleInstance(rdesc, size = nrow(iris))
> 
> rin = makeResampleInstance("CV", iters = 10, task = iris.task)
> 
> 
> 
> cleanEx()
> nameEx("makeStackedLearner")
> ### * makeStackedLearner
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: makeStackedLearner
> ### Title: Create a stacked learner object.
> ### Aliases: makeStackedLearner
> 
> ### ** Examples
> 
> # Classification
> data(iris)
> tsk = makeClassifTask(data = iris, target = "Species")
> base = c("classif.rpart", "classif.lda", "classif.svm")
> lrns = lapply(base, makeLearner)
> lrns = lapply(lrns, setPredictType, "prob")
> m = makeStackedLearner(base.learners = lrns,
+   predict.type = "prob", method = "hill.climb")
> tmp = train(m, tsk)
> res = predict(tmp, tsk)
> 
> # Regression
> data(BostonHousing, package = "mlbench")
> tsk = makeRegrTask(data = BostonHousing, target = "medv")
> base = c("regr.rpart", "regr.svm")
> lrns = lapply(base, makeLearner)
> m = makeStackedLearner(base.learners = lrns,
+   predict.type = "response", method = "compress")
> tmp = train(m, tsk)
# weights:  46
initial  value 932938.622817 
iter  10 value 90720.002169
final  value 90480.904412 
converged
> res = predict(tmp, tsk)
> 
> 
> 
> cleanEx()
> nameEx("makeTuneWrapper")
> ### * makeTuneWrapper
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: makeTuneWrapper
> ### Title: Fuse learner with tuning.
> ### Aliases: makeTuneWrapper
> 
> ### ** Examples
> 
> 
> 
> 
> cleanEx()
> nameEx("makeWeightedClassesWrapper")
> ### * makeWeightedClassesWrapper
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: makeWeightedClassesWrapper
> ### Title: Wraps a classifier for weighted fitting where each class
> ###   receives a weight.
> ### Aliases: makeWeightedClassesWrapper
> 
> ### ** Examples
> 
> 
> 
> 
> cleanEx()
> nameEx("performance")
> ### * performance
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: performance
> ### Title: Measure performance of prediction.
> ### Aliases: performance
> 
> ### ** Examples
> 
> training.set = seq(1, nrow(iris), by = 2)
> test.set = seq(2, nrow(iris), by = 2)
> 
> task = makeClassifTask(data = iris, target = "Species")
> lrn = makeLearner("classif.lda")
> mod = train(lrn, task, subset = training.set)
> pred = predict(mod, newdata = iris[test.set, ])
> performance(pred, measures = mmce)
mmce 
0.04 
> 
> # Compute multiple performance measures at once
> ms = list("mmce" = mmce, "acc" = acc, "timetrain" = timetrain)
> performance(pred, measures = ms, task, mod)
     mmce       acc timetrain 
    0.040     0.960     0.005 
> 
> 
> 
> cleanEx()
> nameEx("plotBMRBoxplots")
> ### * plotBMRBoxplots
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plotBMRBoxplots
> ### Title: Create box or violin plots for a BenchmarkResult.
> ### Aliases: plotBMRBoxplots
> 
> ### ** Examples
> 
> # see benchmark
> 
> 
> 
> cleanEx()
> nameEx("plotBMRRanksAsBarChart")
> ### * plotBMRRanksAsBarChart
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plotBMRRanksAsBarChart
> ### Title: Create a bar chart for ranks in a BenchmarkResult.
> ### Aliases: plotBMRRanksAsBarChart
> 
> ### ** Examples
> 
> # see benchmark
> 
> 
> 
> cleanEx()
> nameEx("plotBMRSummary")
> ### * plotBMRSummary
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plotBMRSummary
> ### Title: Plot a benchmark summary.
> ### Aliases: plotBMRSummary
> 
> ### ** Examples
> 
> # see benchmark
> 
> 
> 
> cleanEx()
> nameEx("plotCalibration")
> ### * plotCalibration
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plotCalibration
> ### Title: Plot calibration data using ggplot2.
> ### Aliases: plotCalibration
> 
> ### ** Examples
> 
> ## Not run: 
> ##D lrns = list(makeLearner("classif.rpart", predict.type = "prob"),
> ##D   makeLearner("classif.nnet", predict.type = "prob"))
> ##D fit = lapply(lrns, train, task = iris.task)
> ##D pred = lapply(fit, predict, task = iris.task)
> ##D names(pred) = c("rpart", "nnet")
> ##D out = generateCalibrationData(pred, groups = 3)
> ##D plotCalibration(out)
> ##D 
> ##D fit = lapply(lrns, train, task = sonar.task)
> ##D pred = lapply(fit, predict, task = sonar.task)
> ##D names(pred) = c("rpart", "lda")
> ##D out = generateCalibrationData(pred)
> ##D plotCalibration(out)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("plotCritDifferences")
> ### * plotCritDifferences
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plotCritDifferences
> ### Title: Plot critical differences for a selected measure.
> ### Aliases: plotCritDifferences
> 
> ### ** Examples
> 
> # see benchmark
> 
> 
> 
> cleanEx()
> nameEx("plotFilterValues")
> ### * plotFilterValues
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plotFilterValues
> ### Title: Plot filter values using ggplot2.
> ### Aliases: plotFilterValues
> 
> ### ** Examples
> 
> fv = generateFilterValuesData(iris.task, method = "variance")
> plotFilterValues(fv)
> 
> 
> 
> cleanEx()
> nameEx("plotHyperParsEffect")
> ### * plotHyperParsEffect
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plotHyperParsEffect
> ### Title: Plot the hyperparameter effects data
> ### Aliases: plotHyperParsEffect
> 
> ### ** Examples
> 
> # see generateHyperParsEffectData
> 
> 
> 
> cleanEx()
> nameEx("plotROCCurves")
> ### * plotROCCurves
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plotROCCurves
> ### Title: Plots a ROC curve using ggplot2.
> ### Aliases: plotROCCurves
> 
> ### ** Examples
> 
> 
> 
> 
> cleanEx()
> nameEx("plotThreshVsPerf")
> ### * plotThreshVsPerf
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plotThreshVsPerf
> ### Title: Plot threshold vs. performance(s) for 2-class classification
> ###   using ggplot2.
> ### Aliases: plotThreshVsPerf
> 
> ### ** Examples
> 
> lrn = makeLearner("classif.rpart", predict.type = "prob")
> mod = train(lrn, sonar.task)
> pred = predict(mod, sonar.task)
> pvs = generateThreshVsPerfData(pred, list(acc, setAggregation(acc, train.mean)))
> plotThreshVsPerf(pvs)
> 
> 
> 
> cleanEx()
> nameEx("plotTuneMultiCritResult")
> ### * plotTuneMultiCritResult
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plotTuneMultiCritResult
> ### Title: Plots multi-criteria results after tuning using ggplot2.
> ### Aliases: plotTuneMultiCritResult
> 
> ### ** Examples
> 
> # see tuneParamsMultiCrit
> 
> 
> 
> cleanEx()
> nameEx("predict.WrappedModel")
> ### * predict.WrappedModel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: predict.WrappedModel
> ### Title: Predict new data.
> ### Aliases: predict.WrappedModel
> 
> ### ** Examples
> 
> # train and predict
> train.set = seq(1, 150, 2)
> test.set = seq(2, 150, 2)
> model = train("classif.lda", iris.task, subset = train.set)
> p = predict(model, newdata = iris, subset = test.set)
> print(p)
Prediction: 75 observations
predict.type: response
threshold: 
time: 0.00
    truth response
2  setosa   setosa
4  setosa   setosa
6  setosa   setosa
8  setosa   setosa
10 setosa   setosa
12 setosa   setosa
... (#rows: 75, #cols: 2)
> predict(model, task = iris.task, subset = test.set)
Prediction: 75 observations
predict.type: response
threshold: 
time: 0.00
   id  truth response
2   2 setosa   setosa
4   4 setosa   setosa
6   6 setosa   setosa
8   8 setosa   setosa
10 10 setosa   setosa
12 12 setosa   setosa
... (#rows: 75, #cols: 3)
> 
> # predict now probabiliies instead of class labels
> lrn = makeLearner("classif.lda", predict.type = "prob")
> model = train(lrn, iris.task, subset = train.set)
> p = predict(model, task = iris.task, subset = test.set)
> print(p)
Prediction: 75 observations
predict.type: prob
threshold: setosa=0.33,versicolor=0.33,virginica=0.33
time: 0.00
   id  truth prob.setosa prob.versicolor prob.virginica response
2   2 setosa           1    1.080106e-16   5.495990e-36   setosa
4   4 setosa           1    3.942221e-16   4.985591e-35   setosa
6   6 setosa           1    1.100758e-20   4.582123e-40   setosa
8   8 setosa           1    1.281324e-19   1.403683e-39   setosa
10 10 setosa           1    3.464605e-18   3.358822e-38   setosa
12 12 setosa           1    1.929046e-18   6.400669e-38   setosa
... (#rows: 75, #cols: 6)
> getPredictionProbabilities(p)
          setosa   versicolor    virginica
2   1.000000e+00 1.080106e-16 5.495990e-36
4   1.000000e+00 3.942221e-16 4.985591e-35
6   1.000000e+00 1.100758e-20 4.582123e-40
8   1.000000e+00 1.281324e-19 1.403683e-39
10  1.000000e+00 3.464605e-18 3.358822e-38
12  1.000000e+00 1.929046e-18 6.400669e-38
14  1.000000e+00 9.174665e-19 6.828749e-39
16  1.000000e+00 4.688969e-27 2.745830e-48
18  1.000000e+00 4.645171e-20 8.128636e-40
20  1.000000e+00 6.694294e-22 5.218579e-42
22  1.000000e+00 1.066840e-19 7.786491e-39
24  1.000000e+00 1.575929e-13 1.372108e-30
26  1.000000e+00 1.644673e-15 2.043591e-34
28  1.000000e+00 6.146076e-21 2.497149e-41
30  1.000000e+00 1.658871e-16 1.808571e-35
32  1.000000e+00 7.487312e-18 9.890059e-37
34  1.000000e+00 2.670831e-28 1.079903e-50
36  1.000000e+00 4.806539e-20 2.339239e-40
38  1.000000e+00 3.813061e-23 2.052406e-44
40  1.000000e+00 7.466350e-20 6.277056e-40
42  1.000000e+00 3.674851e-09 4.536788e-26
44  1.000000e+00 2.376956e-14 3.534795e-31
46  1.000000e+00 4.173036e-15 1.631360e-33
48  1.000000e+00 1.089430e-17 4.863932e-37
50  1.000000e+00 1.774340e-19 1.730362e-39
52  4.334244e-19 9.997549e-01 2.451293e-04
54  1.330724e-22 9.997285e-01 2.714813e-04
56  1.995591e-21 9.997140e-01 2.859941e-04
58  3.839188e-14 9.999999e-01 7.106963e-08
60  1.501079e-20 9.996878e-01 3.122195e-04
62  7.760950e-20 9.995910e-01 4.090414e-04
64  2.080898e-22 9.988291e-01 1.170948e-03
66  3.562231e-17 9.999833e-01 1.673179e-05
68  3.770679e-15 9.999998e-01 1.679949e-07
70  2.965700e-17 9.999987e-01 1.332774e-06
72  6.043746e-17 9.999945e-01 5.493982e-06
74  1.491580e-20 9.999465e-01 5.354890e-05
76  2.932270e-18 9.999654e-01 3.462071e-05
78  2.102664e-26 8.507396e-01 1.492604e-01
80  5.538611e-12 1.000000e+00 1.078961e-08
82  1.436204e-15 9.999999e-01 1.469959e-07
84  2.971359e-30 4.244857e-01 5.755143e-01
86  1.110667e-19 9.983499e-01 1.650103e-03
88  1.466022e-23 9.996693e-01 3.306824e-04
90  6.669272e-21 9.998923e-01 1.076795e-04
92  7.535665e-21 9.995863e-01 4.136941e-04
94  9.307446e-15 9.999999e-01 8.660857e-08
96  3.008115e-16 9.999966e-01 3.390003e-06
98  5.496252e-18 9.999849e-01 1.507050e-05
100 1.363049e-18 9.999717e-01 2.825366e-05
102 4.405357e-37 2.114927e-03 9.978851e-01
104 6.524746e-36 6.511480e-03 9.934885e-01
106 2.777076e-46 4.860808e-06 9.999951e-01
108 3.911272e-39 1.606942e-03 9.983931e-01
110 4.550079e-45 4.040314e-07 9.999996e-01
112 6.617645e-37 3.249953e-03 9.967500e-01
114 1.073113e-40 1.951114e-04 9.998049e-01
116 3.838267e-40 2.724272e-05 9.999728e-01
118 1.307856e-40 2.434837e-05 9.999757e-01
120 1.969144e-32 4.344769e-01 5.655231e-01
122 6.209823e-37 1.068579e-03 9.989314e-01
124 2.370584e-31 1.299622e-01 8.700378e-01
126 1.835130e-33 2.730706e-02 9.726929e-01
128 5.723015e-29 2.605009e-01 7.394991e-01
130 9.802617e-30 5.518685e-01 4.481315e-01
132 8.666451e-33 8.091592e-03 9.919084e-01
134 5.291828e-27 9.385703e-01 6.142969e-02
136 2.222144e-45 3.290807e-06 9.999967e-01
138 1.629560e-32 3.699427e-02 9.630057e-01
140 3.684844e-36 1.254447e-03 9.987456e-01
142 1.587395e-37 2.050417e-04 9.997950e-01
144 1.669608e-44 2.437914e-06 9.999976e-01
146 3.097423e-40 4.263839e-05 9.999574e-01
148 1.441239e-34 5.120744e-03 9.948793e-01
150 1.747306e-31 6.120949e-02 9.387905e-01
> 
> 
> 
> cleanEx()
> nameEx("resample")
> ### * resample
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: resample
> ### Title: Fit models according to a resampling strategy.
> ### Aliases: resample crossval repcv holdout subsample bootstrapOOB
> ###   bootstrapB632 bootstrapB632plus growingcv fixedcv
> 
> ### ** Examples
> 
> task = makeClassifTask(data = iris, target = "Species")
> rdesc = makeResampleDesc("CV", iters = 2)
> r = resample(makeLearner("classif.qda"), task, rdesc)
Resampling: cross-validation
Measures:             mmce      
[Resample] iter 1:    0.0400000 
[Resample] iter 2:    0.0266667 


Aggregated Result: mmce.test.mean=0.0333333


> print(r$aggr)
mmce.test.mean 
    0.03333333 
> print(r$measures.test)
  iter       mmce
1    1 0.04000000
2    2 0.02666667
> print(r$pred)
Resampled Prediction for:
Resample description: cross-validation with 2 iterations.
Predict: test
Stratification: FALSE
predict.type: response
threshold: 
time (mean): 0.00
  id  truth response iter  set
1  2 setosa   setosa    1 test
2  3 setosa   setosa    1 test
3  5 setosa   setosa    1 test
4  6 setosa   setosa    1 test
5  7 setosa   setosa    1 test
6 10 setosa   setosa    1 test
... (#rows: 150, #cols: 5)
> 
> # include the training set performance as well
> rdesc = makeResampleDesc("CV", iters = 2, predict = "both")
> r = resample(makeLearner("classif.qda"), task, rdesc,
+   measures = list(mmce, setAggregation(mmce, train.mean)))
Resampling: cross-validation
Measures:             mmce.train   mmce.test    
[Resample] iter 1:    0.0000000    0.0400000    
[Resample] iter 2:    0.0266667    0.0266667    


Aggregated Result: mmce.test.mean=0.0333333,mmce.train.mean=0.0133333


> print(r$aggr)
 mmce.test.mean mmce.train.mean 
     0.03333333      0.01333333 
> 
> 
> 
> cleanEx()
> nameEx("selectFeatures")
> ### * selectFeatures
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: selectFeatures
> ### Title: Feature selection by wrapper approach.
> ### Aliases: selectFeatures
> 
> ### ** Examples
> 
> 
> 
> 
> cleanEx()
> nameEx("setHyperPars")
> ### * setHyperPars
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: setHyperPars
> ### Title: Set the hyperparameters of a learner object.
> ### Aliases: setHyperPars
> 
> ### ** Examples
> 
> cl1 = makeLearner("classif.ksvm", sigma = 1)
> cl2 = setHyperPars(cl1, sigma = 10, par.vals = list(C = 2))
> print(cl1)
Learner classif.ksvm from package kernlab
Type: classif
Name: Support Vector Machines; Short name: ksvm
Class: classif.ksvm
Properties: twoclass,multiclass,numerics,factors,prob,class.weights
Predict-Type: response
Hyperparameters: fit=FALSE,sigma=1

> # note the now set and altered hyperparameters:
> print(cl2)
Learner classif.ksvm from package kernlab
Type: classif
Name: Support Vector Machines; Short name: ksvm
Class: classif.ksvm
Properties: twoclass,multiclass,numerics,factors,prob,class.weights
Predict-Type: response
Hyperparameters: fit=FALSE,sigma=10,C=2

> 
> 
> 
> cleanEx()
> nameEx("setThreshold")
> ### * setThreshold
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: setThreshold
> ### Title: Set threshold of prediction object.
> ### Aliases: setThreshold
> 
> ### ** Examples
> 
> # create task and train learner (LDA)
> task = makeClassifTask(data = iris, target = "Species")
> lrn = makeLearner("classif.lda", predict.type = "prob")
> mod = train(lrn, task)
> 
> # predict probabilities and compute performance
> pred = predict(mod, newdata = iris)
> performance(pred, measures = mmce)
mmce 
0.02 
> head(as.data.frame(pred))
   truth prob.setosa prob.versicolor prob.virginica response
1 setosa           1    3.896358e-22   2.611168e-42   setosa
2 setosa           1    7.217970e-18   5.042143e-37   setosa
3 setosa           1    1.463849e-19   4.675932e-39   setosa
4 setosa           1    1.268536e-16   3.566610e-35   setosa
5 setosa           1    1.637387e-22   1.082605e-42   setosa
6 setosa           1    3.883282e-21   4.566540e-40   setosa
> 
> # adjust threshold and predict probabilities again
> threshold = c(setosa = 0.4, versicolor = 0.3, virginica = 0.3)
> pred = setThreshold(pred, threshold = threshold)
> performance(pred, measures = mmce)
mmce 
0.02 
> head(as.data.frame(pred))
   truth prob.setosa prob.versicolor prob.virginica response
1 setosa           1    3.896358e-22   2.611168e-42   setosa
2 setosa           1    7.217970e-18   5.042143e-37   setosa
3 setosa           1    1.463849e-19   4.675932e-39   setosa
4 setosa           1    1.268536e-16   3.566610e-35   setosa
5 setosa           1    1.637387e-22   1.082605e-42   setosa
6 setosa           1    3.883282e-21   4.566540e-40   setosa
> 
> 
> 
> cleanEx()
> nameEx("subsetTask")
> ### * subsetTask
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: subsetTask
> ### Title: Subset data in task.
> ### Aliases: subsetTask
> 
> ### ** Examples
> 
> task = makeClassifTask(data = iris, target = "Species")
> subsetTask(task, subset = 1:100)
Supervised task: iris
Type: classif
Target: Species
Observations: 100
Features:
   numerics     factors     ordered functionals 
          4           0           0           0 
Missings: FALSE
Has weights: FALSE
Has blocking: FALSE
Has coordinates: FALSE
Classes: 3
    setosa versicolor  virginica 
        50         50          0 
Positive class: NA
> 
> 
> 
> cleanEx()
> nameEx("summarizeColumns")
> ### * summarizeColumns
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: summarizeColumns
> ### Title: Summarize columns of data.frame or task.
> ### Aliases: summarizeColumns
> 
> ### ** Examples
> 
> summarizeColumns(iris)
          name    type na     mean      disp median     mad  min  max nlevs
1 Sepal.Length numeric  0 5.843333 0.8280661   5.80 1.03782  4.3  7.9     0
2  Sepal.Width numeric  0 3.057333 0.4358663   3.00 0.44478  2.0  4.4     0
3 Petal.Length numeric  0 3.758000 1.7652982   4.35 1.85325  1.0  6.9     0
4  Petal.Width numeric  0 1.199333 0.7622377   1.30 1.03782  0.1  2.5     0
5      Species  factor  0       NA 0.6666667     NA      NA 50.0 50.0     3
> 
> 
> 
> cleanEx()
> nameEx("summarizeLevels")
> ### * summarizeLevels
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: summarizeLevels
> ### Title: Summarizes factors of a data.frame by tabling them.
> ### Aliases: summarizeLevels
> 
> ### ** Examples
> 
> summarizeLevels(iris)
$Species

    setosa versicolor  virginica 
        50         50         50 

> 
> 
> 
> cleanEx()
> nameEx("train")
> ### * train
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: train
> ### Title: Train a learning algorithm.
> ### Aliases: train
> 
> ### ** Examples
> 
> training.set = sample(seq_len(nrow(iris)), nrow(iris) / 2)
> 
> ## use linear discriminant analysis to classify iris data
> task = makeClassifTask(data = iris, target = "Species")
> learner = makeLearner("classif.lda", method = "mle")
> mod = train(learner, task, subset = training.set)
> print(mod)
Model for learner.id=classif.lda; learner.class=classif.lda
Trained on: task.id = iris; obs = 75; features = 4
Hyperparameters: method=mle
> 
> ## use random forest to classify iris data
> task = makeClassifTask(data = iris, target = "Species")
> learner = makeLearner("classif.rpart", minsplit = 7, predict.type = "prob")
> mod = train(learner, task, subset = training.set)
> print(mod)
Model for learner.id=classif.rpart; learner.class=classif.rpart
Trained on: task.id = iris; obs = 75; features = 4
Hyperparameters: xval=0,minsplit=7
> 
> 
> 
> cleanEx()
> nameEx("tuneParams")
> ### * tuneParams
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: tuneParams
> ### Title: Hyperparameter tuning.
> ### Aliases: tuneParams
> 
> ### ** Examples
> 
> set.seed(123)
> # a grid search for an SVM (with a tiny number of points...)
> # note how easily we can optimize on a log-scale
> ps = makeParamSet(
+   makeNumericParam("C", lower = -12, upper = 12, trafo = function(x) 2^x),
+   makeNumericParam("sigma", lower = -12, upper = 12, trafo = function(x) 2^x)
+ )
> ctrl = makeTuneControlGrid(resolution = 2L)
> rdesc = makeResampleDesc("CV", iters = 2L)
> res = tuneParams("classif.ksvm", iris.task, rdesc, par.set = ps, control = ctrl)
[Tune] Started tuning learner classif.ksvm for parameter set:
         Type len Def    Constr Req Tunable Trafo
C     numeric   -   - -12 to 12   -    TRUE     Y
sigma numeric   -   - -12 to 12   -    TRUE     Y
With control class: TuneControlGrid
Imputation value: 1
[Tune-x] 1: C=0.000244; sigma=0.000244
[Tune-y] 1: mmce.test.mean=0.7333333; time: 0.0 min
[Tune-x] 2: C=4.1e+03; sigma=0.000244
[Tune-y] 2: mmce.test.mean=0.0533333; time: 0.0 min
[Tune-x] 3: C=0.000244; sigma=4.1e+03
[Tune-y] 3: mmce.test.mean=0.7333333; time: 0.0 min
[Tune-x] 4: C=4.1e+03; sigma=4.1e+03
[Tune-y] 4: mmce.test.mean=0.7333333; time: 0.0 min
[Tune] Result: C=4.1e+03; sigma=0.000244 : mmce.test.mean=0.0533333
> print(res)
Tune result:
Op. pars: C=4.1e+03; sigma=0.000244
mmce.test.mean=0.0533333
> # access data for all evaluated points
> df = as.data.frame(res$opt.path)
> df1 = as.data.frame(res$opt.path, trafo = TRUE)
> print(head(df[, -ncol(df)]))
    C sigma mmce.test.mean dob eol error.message
1 -12   -12     0.73333333   1  NA          <NA>
2  12   -12     0.05333333   2  NA          <NA>
3 -12    12     0.73333333   3  NA          <NA>
4  12    12     0.73333333   4  NA          <NA>
> print(head(df1[, -ncol(df)]))
    C sigma mmce.test.mean dob eol error.message
1 -12   -12     0.73333333   1  NA          <NA>
2  12   -12     0.05333333   2  NA          <NA>
3 -12    12     0.73333333   3  NA          <NA>
4  12    12     0.73333333   4  NA          <NA>
> # access data for all evaluated points - alternative
> df2 = generateHyperParsEffectData(res)
> df3 = generateHyperParsEffectData(res, trafo = TRUE)
> print(head(df2$data[, -ncol(df2$data)]))
    C sigma mmce.test.mean iteration
1 -12   -12     0.73333333         1
2  12   -12     0.05333333         2
3 -12    12     0.73333333         3
4  12    12     0.73333333         4
> print(head(df3$data[, -ncol(df3$data)]))
             C        sigma mmce.test.mean iteration
1 2.441406e-04 2.441406e-04     0.73333333         1
2 4.096000e+03 2.441406e-04     0.05333333         2
3 2.441406e-04 4.096000e+03     0.73333333         3
4 4.096000e+03 4.096000e+03     0.73333333         4
> ## Not run: 
> ##D # we optimize the SVM over 3 kernels simultanously
> ##D # note how we use dependent params (requires = ...) and iterated F-racing here
> ##D ps = makeParamSet(
> ##D   makeNumericParam("C", lower = -12, upper = 12, trafo = function(x) 2^x),
> ##D   makeDiscreteParam("kernel", values = c("vanilladot", "polydot", "rbfdot")),
> ##D   makeNumericParam("sigma", lower = -12, upper = 12, trafo = function(x) 2^x,
> ##D     requires = quote(kernel == "rbfdot")),
> ##D   makeIntegerParam("degree", lower = 2L, upper = 5L,
> ##D     requires = quote(kernel == "polydot"))
> ##D )
> ##D print(ps)
> ##D ctrl = makeTuneControlIrace(maxExperiments = 5, nbIterations = 1, minNbSurvival = 1)
> ##D rdesc = makeResampleDesc("Holdout")
> ##D res = tuneParams("classif.ksvm", iris.task, rdesc, par.set = ps, control = ctrl)
> ##D print(res)
> ##D df = as.data.frame(res$opt.path)
> ##D print(head(df[, -ncol(df)]))
> ##D 
> ##D # include the training set performance as well
> ##D rdesc = makeResampleDesc("Holdout", predict = "both")
> ##D res = tuneParams("classif.ksvm", iris.task, rdesc, par.set = ps,
> ##D   control = ctrl, measures = list(mmce, setAggregation(mmce, train.mean)))
> ##D print(res)
> ##D df2 = as.data.frame(res$opt.path)
> ##D print(head(df2[, -ncol(df2)]))
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("tuneParamsMultiCrit")
> ### * tuneParamsMultiCrit
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: tuneParamsMultiCrit
> ### Title: Hyperparameter tuning for multiple measures at once.
> ### Aliases: tuneParamsMultiCrit
> 
> ### ** Examples
> 
> 
> 
> 
> ### * <FOOTER>
> ###
> cleanEx()
> options(digits = 7L)
> base::cat("Time elapsed: ", proc.time() - base::get("ptime", pos = 'CheckExEnv'),"\n")
Time elapsed:  76.628 1.929 79.935 0.012 0.023 
> grDevices::dev.off()
null device 
          1 
> ###
> ### Local variables: ***
> ### mode: outline-minor ***
> ### outline-regexp: "\\(> \\)?### [*]+" ***
> ### End: ***
> quit('no')
ERROR: modification of compiler constant of type character, length 2
ERROR: the modified value of the constant is:
[1] "integer"       "integervector"
attr(,".match.hash")
<hash table>
ERROR: the original value of the constant is:
[1] "integer"       "integervector"
ERROR: the modified constant is at index 1
ERROR: the modified constant is in this function body:
c("integer", "integervector")
Fatal error: compiler constants were modified!

